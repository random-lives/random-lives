{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcf0972b-d235-48ce-89a0-5ad0e2d5274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from scipy.interpolate import CubicSpline, interp1d\n",
    "\n",
    "import h5py\n",
    "import rasterio\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f68c9a0c-73eb-406c-8e89-7c31644e94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import CountryData as CD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558cfe0-9989-49bf-88fb-57e8ec773957",
   "metadata": {},
   "source": [
    "# General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddbf631f-5fd1-4148-807d-56b09a314cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = {}\n",
    "FILES = [\"population\", \"cropland\", \"grazing_land\", \"urban_population\"]\n",
    "for i in FILES:\n",
    "    ds = xr.open_dataset('Raw_Data/HYDE34/NetCDF/'+i+'.nc', decode_times=False, chunks={})\n",
    "    hyde[i] = ds[list(ds.data_vars)[0]]  \n",
    "\n",
    "# get the years\n",
    "hyde_years = (hyde[\"population\"].time.values // 365).astype(int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef76287e-5c4d-48bd-8b1f-32f4569e7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open('Raw_Data/HYDE34/general/iso_cr.asc') as src:\n",
    "    iso_data = src.read(1)  # Read first band"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9b7d2-9569-4cee-906f-4476b30d1519",
   "metadata": {},
   "source": [
    "# Processing post-1600 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f702cd8-b1a3-430e-8870-cd4fc7b68836",
   "metadata": {},
   "source": [
    "### Importing Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715df50e-4024-4290-85a9-c77e07b9b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from HYDE data-set, as used in Our World in Data\n",
    "Dataset = {}\n",
    "\n",
    "with open('Raw_Data/Post1600/population.csv', 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the header row\n",
    "    next(csvreader)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    header = \"Afghanistan\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for row in csvreader:\n",
    "        if row[0] == header: \n",
    "            xs += [int(row[2])]\n",
    "            ys += [int(row[3])]\n",
    "        else:\n",
    "            Dataset[header] = CD.Raw_Data(header, xs, ys)\n",
    "            header = row[0]\n",
    "            xs = [int(row[2])]\n",
    "            ys = [int(row[3])]\n",
    "    \n",
    "    Dataset[header] = CD.Raw_Data(header, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71a2a7-fa31-461c-8225-2f9f593bb528",
   "metadata": {},
   "source": [
    "### Filtering countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb50022-bc7f-4dcd-bf05-c646b7c067e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_countries = [\n",
    "    \"Africa\",\n",
    "    \"Asia\",\n",
    "    \"Europe\",\n",
    "    \"European Union (27)\",\n",
    "    \"North America\",\n",
    "    \"Oceania\",\n",
    "    \"South America\",\n",
    "    \"World\"]\n",
    "\n",
    "def is_country(name):\n",
    "    if np.max(Dataset[name].xs)<2021:\n",
    "        return False\n",
    "    if 'UN' in name or 'income' in name:\n",
    "        return False\n",
    "    if name in not_countries:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "Country = {}\n",
    "for i in Dataset:\n",
    "    if is_country(i):\n",
    "        Country[i] = Dataset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bddaab-c01f-45e7-86ba-05ac6eb9ed7c",
   "metadata": {},
   "source": [
    "### Importing Birth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d54a48-cd33-4c25-bfb6-8adf33f06e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Africa (UN)\n",
      "Asia (UN)\n",
      "Europe (UN)\n",
      "High-income countries\n",
      "Land-locked developing countries (LLDC)\n",
      "Latin America and the Caribbean (UN)\n",
      "Least developed countries\n",
      "Less developed regions\n",
      "Less developed regions, excluding China\n",
      "Less developed regions, excluding least developed countries\n",
      "Low-income countries\n",
      "Lower-middle-income countries\n",
      "More developed regions\n",
      "Northern America (UN)\n",
      "Oceania (UN)\n",
      "Small island developing states (SIDS)\n",
      "Upper-middle-income countries\n",
      "Vatican\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "# from Our World in Data (I think?)\n",
    "\n",
    "with open('Raw_Data/Post1600/births.csv', 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the header row\n",
    "    next(csvreader)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    header = \"Afghanistan\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for row in csvreader:\n",
    "        if row[0] == header: \n",
    "            if row[3] != '':\n",
    "                xs += [float(row[2])]\n",
    "                ys += [float(row[3])]\n",
    "        else:\n",
    "            if header in Country and Country[header].xs[-1] == 2021:\n",
    "                Country[header].add_births(xs, ys)\n",
    "            else:\n",
    "                print(header)\n",
    "            header = row[0]\n",
    "            if row[3] != '':\n",
    "                xs = [float(row[2])]\n",
    "                ys = [float(row[3])]\n",
    "    if header in Dataset:\n",
    "        Country[header].add_births(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d35bdd-a0ba-45bc-b483-8cd3cda197e3",
   "metadata": {},
   "source": [
    "### Importing Life Expectancy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af84af0e-0764-4d15-b170-6ff1ce20f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gapminder https://www.gapminder.org/data/documentation/gd004/\n",
    "with open('Raw_Data/Post1600/gm-life-expectancy.csv', 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the header row\n",
    "    next(csvreader)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    header = \"Afghanistan\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for row in csvreader:\n",
    "        if row[1] == header and float(row[2])<2025: \n",
    "            xs += [float(row[2])]\n",
    "            ys += [float(row[3])]\n",
    "        elif row[1]!= header:\n",
    "            if header in Dataset:\n",
    "                Country[header].add_LE(xs, ys)\n",
    "            else:\n",
    "                print(header) \n",
    "            header = row[1]\n",
    "            xs = [float(row[2])]\n",
    "            ys = [float(row[3])]\n",
    "    Country[header].add_LE(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6adaf8ff-1934-4494-8296-a631c7170f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_LE(name,proxy):\n",
    "    Country[name].add_LE(Country[proxy].LE_xs,Country[proxy].LE_ys)\n",
    "\n",
    "LE_proxy_map = {\n",
    "    \"American Samoa\": \"Samoa\",\n",
    "    \"Anguilla\": \"Saint Kitts and Nevis\",\n",
    "    \"Aruba\": \"Barbados\",\n",
    "    \"Falkland Islands\": \"Uruguay\",\n",
    "    \"Faroe Islands\": \"Iceland\",\n",
    "    \"French Guiana\": \"Suriname\",\n",
    "    \"Greenland\": \"Iceland\",\n",
    "    \"Guadeloupe\": \"Barbados\",\n",
    "    \"Martinique\": \"Barbados\",\n",
    "    \"Mayotte\": \"Comoros\",\n",
    "    \"Montserrat\": \"Antigua and Barbuda\",\n",
    "    \"New Caledonia\": \"Fiji\",\n",
    "    \"Puerto Rico\": \"Cuba\",\n",
    "    \"Reunion\": \"Mauritius\",\n",
    "    \"Saint Pierre and Miquelon\": \"France\",  # using France as a mainland proxy\n",
    "    \"Turks and Caicos Islands\": \"Bahamas\",\n",
    "    \"United States Virgin Islands\": \"Barbados\",\n",
    "    \"Western Sahara\": \"Mauritania\"\n",
    "}\n",
    "\n",
    "# Apply the proxy LE data\n",
    "for name, proxy in LE_proxy_map.items():\n",
    "    try:\n",
    "        proxy_LE(name, proxy)\n",
    "    except:\n",
    "        print(name, proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3390a2a-b4b9-49a8-8ebe-3ace5d7084c6",
   "metadata": {},
   "source": [
    "### Importing Statista CBR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494b4ecd-ec46-4738-901b-41b9da93b184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Islands \t 35.89015\n",
      "Greenland \t 34.8115\n",
      "Netherlands Antilles \t 40.522\n",
      "Taiwan \t 40.71235\n"
     ]
    }
   ],
   "source": [
    "# from Statista, which in turn is fitting to \n",
    "\n",
    "with open('Raw_Data/Post1600/CBR.csv', 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the header row\n",
    "    next(csvreader)\n",
    "    \n",
    "    for row in csvreader:\n",
    "        header = row[0]\n",
    "        try:\n",
    "            nums   = np.array([float(i) for i in row[1:]])\n",
    "            Country[header].add_cbr(np.arange(1800,2016), nums)\n",
    "        except: \n",
    "            if row[1]!='':\n",
    "                print(header,'\\t',row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b21c3072-d237-4f89-b97a-fd1c7586dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proxy_CBR(name,proxy):\n",
    "    Country[name].add_cbr(Country[proxy].cbr_xs,Country[proxy].cbr_ys)\n",
    "\n",
    "proxy_CBR('Andorra','Spain')\n",
    "proxy_CBR('Dominica','Saint Vincent and the Grenadines')\n",
    "proxy_CBR('Liechtenstein','Switzerland')\n",
    "proxy_CBR('Saint Kitts and Nevis','Saint Vincent and the Grenadines')\n",
    "proxy_CBR('South Sudan','Sudan')\n",
    "proxy_CBR('Taiwan','South Korea')\n",
    "\n",
    "for name, proxy in LE_proxy_map.items():\n",
    "    if not Country[name].cbr_data:\n",
    "        proxy_CBR(name, proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519c202-bf5b-4582-b090-f57f828e5548",
   "metadata": {},
   "source": [
    "### Importing Country Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7406798a-c75d-40de-84eb-7f27bb49fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British Indian Ocean Territory\n",
      "Christmas Isl.\n",
      "Cocos (Keeling) Isl.\n",
      "Heard and McDonald Isl.\n",
      "Holy See\n",
      "Neth.Antilles\n",
      "Norfolk Island\n",
      "Pitcairn\n",
      "Svalbard & Jan Mayen Isl.\n",
      "Serbia and Montenegro\n",
      "CHE\n",
      "GBR\n"
     ]
    }
   ],
   "source": [
    "with open('Raw_Data/HYDE34/general/country_key.csv', 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the header row\n",
    "    next(csvreader)\n",
    "\n",
    "    for row in csvreader:\n",
    "        name    = row[1].rstrip()\n",
    "        hyde_id = row[0]\n",
    "        try:\n",
    "            Country[name].hyde_id = int(hyde_id)\n",
    "        except: \n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5608e7a3-2771-42e0-a7ce-5b1f67fe3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabeling serbia and montenegro\n",
    "Country['Serbia'].hyde_id = 891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171bbe03-6d84-4799-bb11-987b162c35c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonaire Sint Eustatius and Saba\n",
      "Curacao\n",
      "Guernsey\n",
      "Isle of Man\n",
      "Jersey\n",
      "Kosovo\n",
      "Saint Barthelemy\n",
      "Saint Martin (French part)\n",
      "Sint Maarten (Dutch part)\n",
      "South Sudan\n"
     ]
    }
   ],
   "source": [
    "for i in Country:\n",
    "    if Country[i].hyde_id == None:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d71cf-1681-47f4-8fbb-9ba24d3cd3eb",
   "metadata": {},
   "source": [
    "### Lifetables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6d70ca-1d3f-48c4-b8b0-8c219b527224",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_regions = {\n",
    "        'West': ['United States', 'Canada', 'United Kingdom', 'France', 'Germany',\n",
    "            'Netherlands', 'Belgium', 'Luxembourg', 'Switzerland', 'Austria',\n",
    "            'Australia', 'New Zealand', 'Ireland'],\n",
    "        'North': ['Sweden', 'Norway', 'Denmark', 'Finland', 'Iceland'],\n",
    "        'South': ['Spain', 'Portugal', 'Italy', 'Greece', 'Malta', 'Cyprus'],\n",
    "        'East': ['Poland', 'Czechia', 'Slovakia', 'Hungary', 'Romania',\n",
    "            'Bulgaria', 'Russia', 'Ukraine', 'Belarus', 'Lithuania', \n",
    "            'Latvia', 'Estonia', 'Moldova'],\n",
    "        'Latin': ['Mexico', 'Brazil', 'Argentina', 'Colombia', 'Peru', 'Venezuela',\n",
    "            'Chile', 'Ecuador', 'Guatemala', 'Cuba', 'Bolivia', 'Haiti',\n",
    "            'Dominican Republic', 'Honduras', 'Paraguay', 'Nicaragua',\n",
    "            'El Salvador', 'Costa Rica', 'Panama', 'Uruguay', 'Jamaica'],\n",
    "        'Chilean': ['Chile'],\n",
    "        'South_Asian': ['India', 'Pakistan', 'Bangladesh', 'Sri Lanka', 'Nepal',\n",
    "            'Afghanistan', 'Bhutan', 'Maldives'],\n",
    "        'Far_East_Asian': [\n",
    "            'China', 'Japan', 'South Korea', 'North Korea', 'Taiwan',\n",
    "            'Thailand', 'Vietnam', 'Philippines', 'Indonesia', 'Malaysia',\n",
    "            'Singapore', 'Myanmar', 'Cambodia', 'Laos']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4b7fc8d-33b1-45e9-91a7-47b2e13c5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lt_name in lt_regions:\n",
    "    for country in lt_regions[lt_name]:\n",
    "        Country[country].lifetable = lt_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e6eae-e3f3-4a28-b9ed-58cac8154a9c",
   "metadata": {},
   "source": [
    "## Exporting post-1600 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b95a285a-3845-4b6c-8213-84bbf627b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_1600_Data = {}\n",
    "ID_to_country = {}\n",
    "for i in Country:\n",
    "    entry = Country[i]\n",
    "    if entry.birth_data and entry.cbr_data and entry.LE_data:\n",
    "        Final_1600_Data[i] = CD.Country_Data(entry)\n",
    "        ID_to_country[Final_1600_Data[i].hyde_id] = i\n",
    "    else:\n",
    "        if np.sum(iso_data==entry.hyde_id)>0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76d42366-e2e8-424b-8c7a-2334759c685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('Processed_Data/processed_p1600_data.pkl', 'wb') as f:\n",
    "    dill.dump(Final_1600_Data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb344cde-de2b-4144-b52f-35e333f6d0dd",
   "metadata": {},
   "source": [
    "# Births Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1273d46-d3cd-4e4e-955a-59fda9ec7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f20ec99-a1c2-45f6-9d3f-193e3365f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hunter_gatherer_map(index):\n",
    "    pop  = np.nan_to_num(hyde[\"population\"].isel(time=index).values)\n",
    "    crop    = np.nan_to_num(hyde[\"cropland\"].isel(time=index).values)\n",
    "    grazing = np.nan_to_num(hyde[\"grazing_land\"].isel(time=index).values)\n",
    "    urban   = np.nan_to_num(hyde[\"urban_population\"].isel(time=index).values)\n",
    "    \n",
    "    not_hunt_gather = 1.0*((crop+grazing)>0)*(pop>0)\n",
    "    not_hunt_gather = (uniform_filter(not_hunt_gather, size=15)>0.001) #blurring\n",
    "        \n",
    "    is_hunt_gather = 1-not_hunt_gather\n",
    "    return is_hunt_gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "545cead9-836c-48e8-8ab7-2c66e7ef5e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "hg_array = []\n",
    "for i in range(50):\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    hg_array += [hunter_gatherer_map(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acdc21dd-07b3-4172-8476-e6d0a1ac7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_array = np.array(hg_array)\n",
    "with h5py.File('Processed_Data/hg_array.h5', 'w') as f:\n",
    "    f.create_dataset('data', data=hg_array, compression='gzip', compression_opts=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3293bdb-44c7-4f46-aae1-9dcb8b35c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def births_map(index, year = None):\n",
    "    if year == None:\n",
    "        year = hyde_years[index]\n",
    "    pop  = np.nan_to_num(hyde[\"population\"].isel(time=index).values)\n",
    "    \n",
    "    if year < 1700:\n",
    "        is_hunt_gather = hg_array[index]\n",
    "        \n",
    "        urban   = np.nan_to_num(hyde[\"urban_population\"].isel(time=index).values)\n",
    "        urban_frac = urban/(pop+1)\n",
    "        \n",
    "        #assign a CBR of 33 to hunter-gatherers, 45 to agriculturalists, and 35 for urban areas\n",
    "        cbr_map = (1-is_hunt_gather)*(45-10*urban_frac)+is_hunt_gather*33\n",
    "    else:\n",
    "        cbr_map = 0.0*iso_data\n",
    "        \n",
    "        for i in ID_to_country:\n",
    "            name = ID_to_country[i]\n",
    "            cbr_map += (iso_data==i)*Final_1600_Data[name].CBR_f(year)\n",
    "         \n",
    "    return cbr_map*pop/1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d6991-16d4-4ffb-88ba-ee53499ce6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "births_array = []\n",
    "for i in range(len(hyde_years)):\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    births_array += [births_map(i)]\n",
    "births_array += [births_map(i)] #additional for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0271421-73bd-4dc5-a0da-df4990b43352",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_hyde_years = np.concatenate([hyde_years,[2025]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec11992-3686-4d88-b378-5f6db5e13cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "births_array = np.array(births_array)\n",
    "with h5py.File('Processed_Data/births_array.h5', 'w') as f:\n",
    "    f.create_dataset('data', data=births_array, compression='gzip', compression_opts=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ddf157-80c1-40eb-b88b-f79ab8e4a9fb",
   "metadata": {},
   "source": [
    "# Total Births"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780365e9-77d4-4c3d-a1fd-afcee3296036",
   "metadata": {},
   "source": [
    "## Holocene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b73b3-9879-4a3e-8ffc-4320a5d2470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_vec = np.sum(births_array, axis = (1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd874399-747f-449b-9131-9376cd629399",
   "metadata": {},
   "source": [
    "## Paleolithic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miircppqx2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paleolithic regional population estimates (xs in kya, ys in people)\n",
    "# Sources: Tallavaara et al. 2015 for Europe, archaeological/genetic evidence for others\n",
    "Tallavaara_xs = np.linspace(30, 13, 18)\n",
    "Tallavaara_ys = 1e3 * np.array([330, 310, 320, 250, 210, 190, 170, 140, 140, \n",
    "                                200, 190, 180, 190, 210, 260, 320, 290, 410])\n",
    "\n",
    "HYDE_12kya = {'Africa': 228000, 'Asia': 1184000, 'Europe': 483000, \n",
    "                'North_America': 1179000, 'South_America': 1098000, 'Sahul': 260000}\n",
    "\n",
    "paleo_starting_date = 200 #kya\n",
    "# See Claude chat\n",
    "PaleoData = {\n",
    "    # EUROPE\n",
    "    # Before 50 kya: No H. sapiens in Europe\n",
    "    # 50-45 kya: Initial colonization (Aurignacian), small populations\n",
    "    # 45-30 kya: Gradual increase; Bocquet-Appel et al. suggest meta-pop ~5000 (census ~50-100k)\n",
    "    # 30-13 kya: Tallavaara et al. (2015) detailed estimates\n",
    "    'Europe': {\n",
    "        'xs': np.concatenate(([paleo_starting_date, 50, 45, 40, 35], Tallavaara_xs, [12])),\n",
    "        'ys': np.concatenate((\n",
    "            [0, 0, 30_000, 80_000, 200_000],  # Early Upper Paleolithic\n",
    "            Tallavaara_ys,                      # Tallavaara et al. 30-13 kya\n",
    "            [HYDE_12kya['Europe']]              # HYDE anchor\n",
    "        ))\n",
    "    },\n",
    "    \n",
    "    # AFRICA\n",
    "    # Primary source of H. sapiens; held majority of global population until ~50 kya\n",
    "    # 150 kya: Early H. sapiens, Ne ~10-30k implies census ~100-300k (Sjödin et al. 2012)\n",
    "    # 70 kya: Genetic signal of expansion (Harpending et al. 1998)\n",
    "    # 50 kya: Pre-Out-of-Africa peak\n",
    "    # 20 kya: LGM - some regional contraction but Africa less affected than Eurasia\n",
    "    'Africa': {\n",
    "        'xs': np.array([paleo_starting_date, 150, 100, 70, 50, 30, 20, 12]),\n",
    "        'ys': np.array([\n",
    "            100_000,    # 200 kya - ancestral population (uncertain but Ne×10)\n",
    "            150_000,    # 150 kya - Sjödin et al. census range\n",
    "            200_000,    # 100 kya\n",
    "            350_000,    # 70 kya - expansion signal\n",
    "            450_000,    # 50 kya\n",
    "            400_000,    # 30 kya\n",
    "            300_000,    # 20 kya - LGM\n",
    "            228_000     # 12 kya - HYDE anchor\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    # ASIA (Near East, South Asia, East Asia, Southeast Asia combined)\n",
    "    # Most poorly constrained region; inferred from global totals\n",
    "    # 70 kya: Initial Out-of-Africa populations in Near East/South Asia\n",
    "    # 50 kya: Expansion into East/Southeast Asia\n",
    "    # 30 kya: Pre-LGM population\n",
    "    # 20 kya: Significant LGM bottleneck, especially in northern regions\n",
    "    'Asia': {\n",
    "        'xs': np.array([paleo_starting_date, 75, 70, 50, 40, 30, 20, 12]),\n",
    "        'ys': np.array([\n",
    "            0,           # 200 kya - not yet colonized\n",
    "            0,           # 72 kya - not yet colonized\n",
    "            50_000,      # 70 kya - initial colonization\n",
    "            300_000,     # 50 kya - expansion into E/SE Asia\n",
    "            600_000,     # 40 kya - continued growth\n",
    "            1_000_000,   # 30 kya - pre-LGM\n",
    "            600_000,     # 20 kya - LGM bottleneck (~40% decline)\n",
    "            HYDE_12kya['Asia']  # 12 kya\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    # SAHUL (Australia + New Guinea)\n",
    "    # Colonization ~47 kya based on archaeology and megafauna extinction timing\n",
    "    # van der Kaars et al. (2017): megafauna collapse 45-43 kya implies humans\n",
    "    #   present continent-wide by ~47 kya\n",
    "    # Gautney & Holliday (2015): ~47k at LGM using carnivore density method\n",
    "    # Low carrying capacity: vast arid interior, only fully occupied in Holocene\n",
    "    'Sahul': {\n",
    "        'xs': np.array([paleo_starting_date, 50, 47, 45, 35, 25, 20, 12]),\n",
    "        'ys': np.array([\n",
    "            0,          # 200 kya - not colonized\n",
    "            0,          # 50 kya - not yet colonized\n",
    "            5_000,      # 47 kya - initial colonization\n",
    "            30_000,     # 45 kya - continent-wide dispersal (megafauna extinction)\n",
    "            70_000,     # 35 kya - pre-LGM growth\n",
    "            50_000,     # 25 kya - early LGM contraction\n",
    "            40_000,     # 20 kya - LGM minimum; interior largely abandoned\n",
    "            HYDE_12kya['Sahul']  # 12 kya - recovery underway\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    # NORTH AMERICA\n",
    "    # Entry ~16 kya via coastal route (possibly ice-free corridor slightly later)\n",
    "    # Initial founding population small: ~1-5k effective, ~5-20k census\n",
    "    # Rapid expansion: Clovis appears ~13.1 kya, continent-wide by ~12.5 kya\n",
    "    # Fagundes et al. (2008), Kitchen et al. (2008)\n",
    "    'North_America': {\n",
    "        'xs': np.array([paleo_starting_date, 20, 16, 15, 14, 13, 12]),\n",
    "        'ys': np.array([\n",
    "            0,           # 150 kya - not colonized\n",
    "            0,           # 20 kya - not colonized\n",
    "            5_000,       # 16 kya - initial entry\n",
    "            20_000,      # 15 kya - early expansion\n",
    "            60_000,      # 14 kya - continued expansion\n",
    "            200_000,     # 13 kya - Clovis period\n",
    "            HYDE_12kya['North_America']  # 12 kya\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    # SOUTH AMERICA\n",
    "    # Entry slightly after North America; Monte Verde ~14.5 kya\n",
    "    # Rapid expansion down Pacific coast\n",
    "    'South_America': {\n",
    "        'xs': np.array([paleo_starting_date, 16, 14.5, 14, 13, 12]),\n",
    "        'ys': np.array([\n",
    "            0,           # 150 kya - not colonized\n",
    "            0,           # 16 kya - not yet reached\n",
    "            2_000,       # 14.5 kya - initial entry (Monte Verde)\n",
    "            20_000,      # 14 kya - early expansion\n",
    "            150_000,     # 13 kya - rapid growth\n",
    "            HYDE_12kya['South_America']  # 12 kya\n",
    "        ])\n",
    "    },\n",
    "}\n",
    "\n",
    "# Calculate Paleolithic births per year at each time point\n",
    "paleo_kya = np.linspace(paleo_starting_date, 12, 500)  # 150 kya to 12 kya\n",
    "paleo_years = -1000 * paleo_kya + 2000  # convert to calendar year\n",
    "\n",
    "paleo_world_pop = sum(np.interp(paleo_kya, PaleoData[r]['xs'][::-1], PaleoData[r]['ys'][::-1]) \n",
    "                      for r in PaleoData)\n",
    "paleo_births_per_year = paleo_world_pop / 30  # ~30 year generation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d02a7-5fcf-4c31-b6c7-162c7c91bb79",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74kxmhmokwm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unified birth series (Paleolithic + Holocene)\n",
    "transition_year = hyde_years[0]  # -10000\n",
    "\n",
    "# Trim paleolithic to not overlap, combine with HYDE\n",
    "paleo_mask = paleo_years < transition_year\n",
    "data_years = np.concatenate([paleo_years[paleo_mask], extended_hyde_years.astype(float)])\n",
    "data_births = np.concatenate([paleo_births_per_year[paleo_mask], birth_vec])\n",
    "\n",
    "# Interpolate birth rate at every year, then cumsum\n",
    "# This assumes linear interpolation between data points (not step function)\n",
    "all_years = np.arange(int(data_years[0]), int(data_years[-1]) + 1)\n",
    "births_per_year = np.interp(all_years, data_years, data_births)\n",
    "cumulative_births = np.cumsum(births_per_year)\n",
    "\n",
    "print(f\"Total human births: {cumulative_births[-1]/1e9:.1f} billion\")\n",
    "print(f\"  Paleolithic: {cumulative_births[all_years < transition_year][-1]/1e9:.1f}B\")\n",
    "print(f\"  Holocene: {(cumulative_births[-1] - cumulative_births[all_years < transition_year][-1])/1e9:.1f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fqdmarrwoz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export unified births and paleolithic regional data\n",
    "with open('Processed_Data/unified_births.pkl', 'wb') as f:\n",
    "    dill.dump({\n",
    "        'years': all_years,\n",
    "        'cumulative_births': cumulative_births,\n",
    "        'total_births': cumulative_births[-1],\n",
    "        'transition_year': transition_year,\n",
    "    }, f)\n",
    "\n",
    "with open('Processed_Data/paleo_regions.pkl', 'wb') as f:\n",
    "    dill.dump({'regions': list(PaleoData.keys()), 'regional_data': PaleoData}, f)\n",
    "\n",
    "print(\"Saved unified_births.pkl and paleo_regions.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xszgnhqn94",
   "metadata": {},
   "source": [
    "# Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b849f-e466-44d1-a13a-3abc59c385b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_years,cumulative_births)\n",
    "plt.yscale('log')\n",
    "#plt.xlim(-4e3,2e3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aacac0-91e9-49b0-a9ef-c4c86d13177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(paleo_years,paleo_world_pop)\n",
    "plt.yscale('log')\n",
    "#plt.xlim(-4e3,2e3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc70b86-0ba0-4784-8ef3-5865abdf1079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0eb13b-56cf-429c-a576-f5282e8db065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
